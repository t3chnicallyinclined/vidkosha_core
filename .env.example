# Copy this file to `.env` and tweak values as needed.
# These settings let the Rust CLI talk to the local vLLM server that
# serves `meta-llama/Llama-3.1-8B-Instruct` via the OpenAI-compatible API.

# Model alias registered when the server is started with
# `--served-model-name llama3-local`.
VK_CORTEX_LLM_MODEL=llama3-local

# Base URL for the OpenAI-compatible endpoint exposed by vLLM.
OPENAI_BASE_URL=http://127.0.0.1:8000/v1

# vLLM does not validate the key, but the OpenAI SDK expects one.
OPENAI_API_KEY=sk-local

# --- Helix AI Fabric ---
# HelixDB (graph + vector store) base URL. Default uses the local binary on port 6969.
HELIX_BASE_URL=http://127.0.0.1:6969
# API token used for authenticated Helix calls. Generate with `helix user token` or set dev value.
HELIX_API_TOKEN=helix-local-token
# Namespace / graph slug the agents should target. Helix uses collections to partition data.
HELIX_GRAPH_NAMESPACE=vidkosha_cortex
# Optional timeout override for Helix HTTP calls (milliseconds).
# HELIX_HTTP_TIMEOUT_MS=10000

# Embeddings backend (Helix still calls out to this dedicated embeddings server).
RAG_EMBEDDING_MODEL=bge-m3
RAG_EMBEDDING_BASE_URL=http://127.0.0.1:9000/v1
RAG_EMBEDDING_API_KEY=sk-local
RAG_VECTOR_DIM=1024

# HelixQL query names (v2 by default)
HELIX_WRITE_QUERY=write_memory_v2
HELIX_SEARCH_QUERY=search_memory_v2
HELIX_DELETE_QUERY=delete_memory_v2
HELIX_RICH_WRITE_QUERY=write_memory_v2
